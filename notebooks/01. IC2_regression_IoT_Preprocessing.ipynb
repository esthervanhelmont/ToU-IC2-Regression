{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e65ee4c",
   "metadata": {},
   "source": [
    "## 1. Load and Inspect the Data\n",
    "\n",
    "1. First, we load the IoT network dataset from a database file. We also tidy up the column names (make them lowercase and remove spaces/dots) so they're easier to work with. Finally, we print out the shape of the dataset (how many rows and columns) to see its size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88d8fbeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (123117, 84)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, sqlite3\n",
    "from pathlib import Path\n",
    "\n",
    "# Load the entire dataset from the SQLite database\n",
    "with sqlite3.connect(Path(\"../data/raw/rt_iot2022.db\")) as conn:\n",
    "    df = pd.read_sql(\"SELECT * FROM flows\", conn)\n",
    "\n",
    "# Clean column names: lowercase, no spaces or dots\n",
    "df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('.', '_')\n",
    "\n",
    "# Show the dataset shape (number of rows, number of columns)\n",
    "print(\"Dataset shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b3f5f3",
   "metadata": {},
   "source": [
    "For a quick overview, we list each column name and what type of data it holds (integer, float, object, etc.). This helps us understand the kinds of features we have – for example, which are numeric counts, which are text categories, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15e2e0f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id_orig_p: int64\n",
      "id_resp_p: int64\n",
      "proto: object\n",
      "service: object\n",
      "flow_duration: float64\n",
      "fwd_pkts_tot: int64\n",
      "bwd_pkts_tot: int64\n",
      "fwd_data_pkts_tot: int64\n",
      "bwd_data_pkts_tot: int64\n",
      "fwd_pkts_per_sec: float64\n",
      "bwd_pkts_per_sec: float64\n",
      "flow_pkts_per_sec: float64\n",
      "down_up_ratio: float64\n",
      "fwd_header_size_tot: int64\n",
      "fwd_header_size_min: int64\n",
      "fwd_header_size_max: int64\n",
      "bwd_header_size_tot: int64\n",
      "bwd_header_size_min: int64\n",
      "bwd_header_size_max: int64\n",
      "flow_fin_flag_count: int64\n",
      "flow_syn_flag_count: int64\n",
      "flow_rst_flag_count: int64\n",
      "fwd_psh_flag_count: int64\n",
      "bwd_psh_flag_count: int64\n",
      "flow_ack_flag_count: int64\n",
      "fwd_urg_flag_count: int64\n",
      "bwd_urg_flag_count: int64\n",
      "flow_cwr_flag_count: int64\n",
      "flow_ece_flag_count: int64\n",
      "fwd_pkts_payload_min: int64\n",
      "fwd_pkts_payload_max: int64\n",
      "fwd_pkts_payload_tot: int64\n",
      "fwd_pkts_payload_avg: float64\n",
      "fwd_pkts_payload_std: float64\n",
      "bwd_pkts_payload_min: int64\n",
      "bwd_pkts_payload_max: int64\n",
      "bwd_pkts_payload_tot: int64\n",
      "bwd_pkts_payload_avg: float64\n",
      "bwd_pkts_payload_std: float64\n",
      "flow_pkts_payload_min: int64\n",
      "flow_pkts_payload_max: int64\n",
      "flow_pkts_payload_tot: int64\n",
      "flow_pkts_payload_avg: float64\n",
      "flow_pkts_payload_std: float64\n",
      "fwd_iat_min: float64\n",
      "fwd_iat_max: float64\n",
      "fwd_iat_tot: float64\n",
      "fwd_iat_avg: float64\n",
      "fwd_iat_std: float64\n",
      "bwd_iat_min: float64\n",
      "bwd_iat_max: float64\n",
      "bwd_iat_tot: float64\n",
      "bwd_iat_avg: float64\n",
      "bwd_iat_std: float64\n",
      "flow_iat_min: float64\n",
      "flow_iat_max: float64\n",
      "flow_iat_tot: float64\n",
      "flow_iat_avg: float64\n",
      "flow_iat_std: float64\n",
      "payload_bytes_per_second: float64\n",
      "fwd_subflow_pkts: float64\n",
      "bwd_subflow_pkts: float64\n",
      "fwd_subflow_bytes: float64\n",
      "bwd_subflow_bytes: float64\n",
      "fwd_bulk_bytes: float64\n",
      "bwd_bulk_bytes: float64\n",
      "fwd_bulk_packets: float64\n",
      "bwd_bulk_packets: float64\n",
      "fwd_bulk_rate: float64\n",
      "bwd_bulk_rate: float64\n",
      "active_min: float64\n",
      "active_max: float64\n",
      "active_tot: float64\n",
      "active_avg: float64\n",
      "active_std: float64\n",
      "idle_min: float64\n",
      "idle_max: float64\n",
      "idle_tot: float64\n",
      "idle_avg: float64\n",
      "idle_std: float64\n",
      "fwd_init_window_size: int64\n",
      "bwd_init_window_size: int64\n",
      "fwd_last_window_size: int64\n",
      "attack_type: object\n"
     ]
    }
   ],
   "source": [
    "# Print each column's name and data type\n",
    "for col, dtype in df.dtypes.items():\n",
    "    print(f\"{col}: {dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e5fee1",
   "metadata": {},
   "source": [
    "## 2. Remove Unnecessary Columns\n",
    "\n",
    "1. Some columns in the data aren't useful for prediction, or could even mislead the model:\n",
    "    - **Identifiers** like source or destination port numbers (`id_orig_p`, `id_resp_p`) don’t help predict duration (they're more like IDs, not behaviors).\n",
    "    - **Rate features** such as `fwd_pkts_per_sec` or `flow_pkts_per_sec` are calculated using `flow_duration` itself – including them would be like giving the model the answer (this is called target leakage).\n",
    "    - **Attack label** (`attack_type`) is a label for classification (attack vs. normal). We are doing regression to predict duration, so we don’t use the attack labels here.\n",
    "\n",
    "We remove these columns to make the dataset smaller and prevent any leakage “cheating” or noise from irrelevant data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c75006a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After drop: (123117, 77)\n"
     ]
    }
   ],
   "source": [
    "# Drop columns that are IDs, leak target info, or are not needed for regression\n",
    "cols_to_drop = [\n",
    "    \"id_orig_p\", \"id_resp_p\",           # connection identifiers (ports)\n",
    "    \"fwd_pkts_per_sec\", \"bwd_pkts_per_sec\", \"flow_pkts_per_sec\", \"payload_bytes_per_second\",  # rate features (target leakage)\n",
    "    \"attack_type\"                      # attack label (not used in predicting duration)\n",
    "]\n",
    "df_clean = df.drop(columns=[c for c in cols_to_drop if c in df.columns])\n",
    "print(\"After drop:\", df_clean.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26b02e8",
   "metadata": {},
   "source": [
    "(After this drop, the dataset still has all the features we need, but without those unhelpful columns.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b23754",
   "metadata": {},
   "source": [
    "## 3. Detect Outliers\n",
    "\n",
    "1. Now we check each numeric feature for **outliers** – extreme values that are much higher or lower than most of the data. We use the **IQR (Interquartile Range)** method:\n",
    "    - For each feature, find Q1 (25th percentile) and Q3 (75th percentile).\n",
    "    - Compute IQR = Q3 – Q1, which is the range of the middle 50% of values.\n",
    "    - Flag any value that is below `Q1 - 1.5*IQR` or above `Q3 + 1.5*IQR` as an outlier.\n",
    "\n",
    "We won't remove data just for being an outlier here; we just want to **identify** which features have a lot of outliers. This can tell us if a feature has a very **long tail** or unusual distribution. Knowing this, we might later apply transformations (like taking a log) or handle these features carefully so that a few extreme values don’t distort our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "533c1b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flow_pkts_payload_avg: 31.0% outliers\n",
      "bwd_header_size_tot: 26.9% outliers\n",
      "bwd_header_size_max: 26.9% outliers\n",
      "flow_ack_flag_count: 26.9% outliers\n",
      "bwd_pkts_tot: 26.3% outliers\n"
     ]
    }
   ],
   "source": [
    "# Calculate outlier share for numeric features (excluding the target if present)\n",
    "numeric_cols = df_clean.select_dtypes(include='number').columns\n",
    "outlier_share = {}\n",
    "for col in numeric_cols:\n",
    "    if col == 'flow_duration':\n",
    "        continue\n",
    "    Q1 = df_clean[col].quantile(0.25)\n",
    "    Q3 = df_clean[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    # Fraction of rows that are outliers in this feature\n",
    "    share = ((df_clean[col] < lower_bound) | (df_clean[col] > upper_bound)).mean()\n",
    "    outlier_share[col] = share\n",
    "\n",
    "# Print the top 5 features by outlier frequency\n",
    "top_outliers = sorted(outlier_share.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "for col, share in top_outliers:\n",
    "    print(f\"{col}: {share*100:.1f}% outliers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9032d8",
   "metadata": {},
   "source": [
    "*The code above will list a few features with the highest percentage of outliers. For instance, we might see something like `flow_pkts_payload_avg: 31.3% outliers`, meaning about 31% of the rows have values in `flow_pkts_payload_avg` that are quite extreme. This tells us that feature has a **long tail** of unusual values.*\n",
    "\n",
    "*(We are just observing these; no automatic removal is done at this point.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7401083",
   "metadata": {},
   "source": [
    "## 4. Remove Almost-Constant Features\n",
    "\n",
    "1. With many features, some might be **almost constant** – they hardly vary across the dataset. For example, imagine a column that's `0` for 99.9% of the entries and `1` for a few others; it doesn’t carry much information. Such features can safely be removed:\n",
    "    - We consider features that have very few unique values (e.g. 3 or fewer unique values in all 100k+ rows).\n",
    "    - Or if one value dominates the column (for instance, the same value appears in ≥98.5% of the rows).\n",
    "    - These columns add little to no predictive signal and can even act as noise.\n",
    "\n",
    "Removing them reduces the “dimension” of our data, making training faster and the model less likely to get confused by meaningless data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7610c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 13 quasi-constant features.\n",
      "Remaining shape: (123117, 64)\n"
     ]
    }
   ],
   "source": [
    "# Identify quasi-constant features (very low variability)\n",
    "quasi_cols = []\n",
    "for col in df_clean.columns:\n",
    "    if col == 'flow_duration':\n",
    "        continue\n",
    "    values = df_clean[col]\n",
    "    top_freq = values.value_counts(normalize=True, dropna=False).max()\n",
    "    if values.nunique(dropna=False) <= 3 or top_freq >= 0.985:\n",
    "        quasi_cols.append(col)\n",
    "\n",
    "# Drop quasi-constant features\n",
    "df_clean = df_clean.drop(columns=quasi_cols)\n",
    "print(f\"Dropped {len(quasi_cols)} quasi-constant features.\")\n",
    "print(\"Remaining shape:\", df_clean.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa996a00",
   "metadata": {},
   "source": [
    "(The code flags features with ≤3 unique values, or where one value appears in ≥98.5% of rows, as quasi-constant. We then drop them. Suppose it drops 12 columns; that will be reported. This step helps us by eliminating columns that were almost always the same, which do not help the model learn anything distinct.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac13c3b9",
   "metadata": {},
   "source": [
    "## 5. Remove Constant Features\n",
    "\n",
    "1. A **constant** feature is an even simpler case: it has **exactly one** value for every row (no variation at all). This feature has zero predictive power (it’s the same for everything!). We scan for any constant columns and drop them if found:\n",
    "    - This ensures we only keep features that have at least some variation.\n",
    "    - (Often, constant columns would already be caught by our quasi-constant check above, but we double-check just in case.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74ed1523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constant columns: []\n"
     ]
    }
   ],
   "source": [
    "# Check for any constant columns remaining\n",
    "const_cols = [c for c in df_clean.columns if df_clean[c].nunique(dropna=False) == 1]\n",
    "print(\"Constant columns:\", const_cols)\n",
    "if const_cols:\n",
    "    df_clean = df_clean.drop(columns=const_cols)\n",
    "    print(\"Dropped constant features. New shape:\", df_clean.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60570a1",
   "metadata": {},
   "source": [
    "*Usually, after the previous steps, there should be **0 constant columns** left (the list will be empty). If there were any, this code would remove them and print the updated shape.*\n",
    "\n",
    "*(By removing constant and quasi-constant features, we ensure that every column left has some variability that could be useful for prediction.)*\n",
    "\n",
    "## 6. Remove Duplicate Rows\n",
    "\n",
    "1. Next, we look for **duplicate rows** – exact copies of data entries. Duplicates can happen in data collection and can bias the model (it would effectively see the same example twice and might give it more weight).\n",
    "    - We remove duplicate entries to ensure each flow appears only once. This way, each training example is unique.\n",
    "    - This also dramatically reduces the dataset size if many duplicates exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a616e126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 104847 duplicate rows. New shape: (18270, 64)\n"
     ]
    }
   ],
   "source": [
    "initial_rows = df_clean.shape[0]\n",
    "df_clean = df_clean.drop_duplicates().reset_index(drop=True)\n",
    "removed = initial_rows - df_clean.shape[0]\n",
    "print(f\"Removed {removed} duplicate rows. New shape:\", df_clean.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f8e649",
   "metadata": {},
   "source": [
    "*In our dataset, it turns out **a lot** of rows were exact duplicates (the output will show how many were removed). For example, if it prints “Removed 99793 duplicate rows,” that means initially we had many repeated flows and now we’re down to only unique ones. After removing duplicates, our dataset might have around 18k unique rows left (just as an example).*\n",
    "\n",
    "*(Removing duplicates is important so the model doesn't get a false sense of confidence by seeing the same thing multiple times.)*\n",
    "\n",
    "## 7. Drop Highly Correlated Features\n",
    "\n",
    "1. Sometimes two features are **highly correlated**, meaning they essentially contain the same information. For example, `fwd_pkts_tot` (forward packet count) and `bwd_pkts_tot` (backward packet count) might individually not be perfectly correlated, but imagine if we had `total_pkts` as another column which is just their sum – `total_pkts` would be very strongly correlated with those. Including all of them can be redundant and might confuse or overfit certain models (especially linear models which don’t handle multicollinearity well).\n",
    "    - We compute the correlation between every pair of numeric features.\n",
    "    - If a pair of features has a correlation above 0.95 (very similar behavior), we drop one of them.\n",
    "    - This reduces redundancy. It’s like if two sensors always give the same reading, you only need one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae19a0be",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m upper_mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtriu(np\u001b[38;5;241m.\u001b[39mones_like(corr_matrix, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m), k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Find features with correlation > 0.95\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m to_drop_corr \u001b[38;5;241m=\u001b[39m [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m corr_matrix\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(corr_matrix[col][upper_mask] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.95\u001b[39m)]\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Drop these highly correlated features\u001b[39;00m\n\u001b[1;32m     12\u001b[0m df_clean \u001b[38;5;241m=\u001b[39m df_clean\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39mto_drop_corr)\n",
      "Cell \u001b[0;32mIn[16], line 9\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      6\u001b[0m upper_mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtriu(np\u001b[38;5;241m.\u001b[39mones_like(corr_matrix, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m), k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Find features with correlation > 0.95\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m to_drop_corr \u001b[38;5;241m=\u001b[39m [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m corr_matrix\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[43mcorr_matrix\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mupper_mask\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.95\u001b[39m)]\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Drop these highly correlated features\u001b[39;00m\n\u001b[1;32m     12\u001b[0m df_clean \u001b[38;5;241m=\u001b[39m df_clean\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39mto_drop_corr)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/series.py:1160\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1158\u001b[0m     key \u001b[38;5;241m=\u001b[39m check_bool_indexer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, key)\n\u001b[1;32m   1159\u001b[0m     key \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(key, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[0;32m-> 1160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_rows_with_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_with(key)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/series.py:1226\u001b[0m, in \u001b[0;36mSeries._get_rows_with_mask\u001b[0;34m(self, indexer)\u001b[0m\n\u001b[1;32m   1225\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_rows_with_mask\u001b[39m(\u001b[38;5;28mself\u001b[39m, indexer: npt\u001b[38;5;241m.\u001b[39mNDArray[np\u001b[38;5;241m.\u001b[39mbool_]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series:\n\u001b[0;32m-> 1226\u001b[0m     new_mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_rows_with_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1227\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(new_mgr, axes\u001b[38;5;241m=\u001b[39mnew_mgr\u001b[38;5;241m.\u001b[39maxes)\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/internals/managers.py:1959\u001b[0m, in \u001b[0;36mSingleBlockManager.get_rows_with_mask\u001b[0;34m(self, indexer)\u001b[0m\n\u001b[1;32m   1957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m using_copy_on_write() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(indexer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m indexer\u001b[38;5;241m.\u001b[39mall():\n\u001b[1;32m   1958\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)(blk\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m-> 1959\u001b[0m array \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1961\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(indexer, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mand\u001b[39;00m indexer\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1962\u001b[0m     \u001b[38;5;66;03m# boolean indexing always gives a copy with numpy\u001b[39;00m\n\u001b[1;32m   1963\u001b[0m     refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Compute absolute correlation matrix for numeric features\n",
    "corr_matrix = df_clean.corr(numeric_only=True).abs()\n",
    "# Create a mask for the upper triangle of the matrix (to avoid duplicate checks)\n",
    "upper_mask = np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)\n",
    "\n",
    "# Find features with correlation > 0.95\n",
    "to_drop_corr = [col for col in corr_matrix.columns if any(corr_matrix[col][upper_mask] > 0.95)]\n",
    "\n",
    "# Drop these highly correlated features\n",
    "df_clean = df_clean.drop(columns=to_drop_corr)\n",
    "print(\"Dropped highly correlated features:\", to_drop_corr)\n",
    "print(\"New shape:\", df_clean.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87750395",
   "metadata": {},
   "source": [
    "*This code scans the correlation matrix and drops any feature that is extremely (≥95%) correlated with another. We print which columns were dropped due to high correlation.*\n",
    "\n",
    "*For example, if `bwd_pkts_tot` and `flow_pkts_payload_tot` have a 0.99 correlation, one of them will be dropped. By doing this, we keep only one representative from any group of super-similar features. This helps simplify the data without losing much information.*\n",
    "\n",
    "## 8. Simplify Categorical Features\n",
    "\n",
    "1. The dataset has some **categorical features** (`proto` for protocol type, and `service` for service/application type). These are text labels. There could be many unique values, some appearing very rarely. For modeling, it’s better to reduce the number of categories:\n",
    "    - We group **rare categories** into an \"other\" category. For example, if `service` has 15 possible values but 5 of them only appear a few times, we label those 5 as \"other.\"\n",
    "    - Specifically, we'll keep the **top 10 most frequent** values for each categorical feature and replace the rest with `\"other\"`.\n",
    "    - This way, our model focuses on the common categories and doesn't get bogged down by very infrequent ones (which might not provide reliable patterns).\n",
    "\n",
    "This is like saying, for the purpose of learning, we distinguish the major types and lump all miscellaneous minor types together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e86241d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['proto', 'service']:\n",
    "    if col in df_clean.columns:\n",
    "        top10 = df_clean[col].value_counts().nlargest(10).index\n",
    "        df_clean[col] = df_clean[col].where(df_clean[col].isin(top10), 'other')\n",
    "\n",
    "if 'proto' in df_clean.columns:\n",
    "    print(\"Unique protocols after simplification:\", df_clean['proto'].nunique())\n",
    "if 'service' in df_clean.columns:\n",
    "    print(\"Unique services after simplification:\", df_clean['service'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0dea45",
   "metadata": {},
   "source": [
    "*After this, the `proto` column might have a few unique values (for example: `\"tcp\"`, `\"udp\"`, and `\"other\"` if those were the main ones) instead of possibly more. The `service` column will also be reduced to its top categories plus `\"other\"`. This simplification makes our next steps (like one-hot encoding for machine learning) more manageable.*\n",
    "\n",
    "## 9. Feature Engineering\n",
    "\n",
    "So far, we have only **cleaned and filtered** the raw features. The next step is to create **new, informative features** that might capture the patterns in the data better than the raw features alone. This is called **feature engineering**. We will combine or transform existing columns to highlight important aspects of the network flow.\n",
    "\n",
    "Why do this? Some relationships or behaviors are not directly given by a single raw column. By creating new features, we help the model by giving it more meaningful signals to learn from (instead of expecting the model to discover these signals on its own from the raw data).\n",
    "\n",
    "Below is a table summarizing the new features we will create, which source columns they come from, and **why** they are useful:\n",
    "\n",
    "| New Column | Source Columns | Why we do this |\n",
    "| --- | --- | --- |\n",
    "| **pkts_tot** | fwd_pkts_tot, bwd_pkts_tot | Total number of packets in the flow (forward + backward). This gives the overall size of the flow in terms of packet count. |\n",
    "| **data_pkts_tot** | fwd_data_pkts_tot, bwd_data_pkts_tot | Total **data-carrying** packets (both directions). This focuses on packets that carry payload, indicating the volume of actual data transfer. |\n",
    "| **hdr_size_tot_sum** | fwd_header_size_tot, bwd_header_size_tot | Total header bytes from both directions. This measures the total overhead size of the flow. |\n",
    "| **X_ratio_fb** (e.g., **pkts_ratio_fb**, **data_pkts_ratio_fb**, **hdr_size_ratio_fb**) | forward vs. backward counts (packets, data packets, header bytes) | The ratio of forward to backward values. It shows if traffic was balanced or mostly one-sided. For example, if **pkts_ratio_fb** ≫ 1, the client sent many more packets than it received (mostly one-way communication). A ratio ≈ 1 means a balanced two-way exchange. |\n",
    "| **X_symmetry** (e.g., **pkts_symmetry**, **hdr_size_symmetry**) | forward vs. backward values (packets, header bytes) | A different way to measure balance: `(fwd - bwd) / (fwd + bwd)`. This ranges from -1 to +1. **0** means perfectly symmetric (equal forward and backward), **+0.8** (for example) would mean heavily skewed to forward, **-0.8** skewed to backward. This feature captures direction *bias* in the communication. |\n",
    "| **fwd_iat_cv** (and **bwd_iat_cv**, **flow_iat_cv**) | X_iat_std, X_iat_avg (for forward, backward, overall flow) | **Coefficient of Variation** of Inter-Arrival Time: basically `std/avg` of packet timing gaps. This measures **burstiness** of traffic. A high CV means packet arrivals are erratic (some quick bursts and some long pauses), whereas a low CV means packets come at a steady rate. |\n",
    "| **active_share** | active_tot, idle_tot | Fraction of time the flow was active: `active_tot / (active_tot + idle_tot)`. This tells us if a flow spent most of its life transmitting data or waiting. **active_share** near 1 means the flow was busy almost the whole time. |\n",
    "| **idle_share** | active_tot, idle_tot | Fraction of time the flow was idle (complement of active_share). If idle_share is high, the flow was mostly waiting around. |\n",
    "| **flag_count_density** (for each TCP flag type, e.g., **flow_syn_flag_count_density**) | That flag’s count, and pkts_tot | We take counts of certain TCP flags (SYN, ACK, FIN, etc.) and divide by total packets. This gives the **percentage of packets** that had each flag. It normalizes flag counts by flow length so we can compare flows of different lengths. (For example, “30% of packets in this flow were SYN packets” is a meaningful ratio.) Flags can indicate connection setup, teardown, or unusual events in the flow. |\n",
    "| **fwd_window_delta** | fwd_init_window_size, fwd_last_window_size | Change in the TCP window size (forward direction) from start to end of the flow. This shows how the sender’s TCP buffer window evolved. A large change might indicate network congestion control kicking in or other network behavior. |\n",
    "| **proto_top** | proto | Simplified **protocol** category. This is just the `proto` feature we already simplified (grouped minor ones into \"other\"). It’s a categorical feature we will use in modeling (via one-hot encoding). |\n",
    "| **service_top** | service | Simplified **service** category (same idea as proto_top). |\n",
    "| **log1p__X** *(optional)* | any skewed numeric column X | The natural log of (1 + X). This is a transformation we might apply to very skewed features (just like we'll do for the target) to reduce their range. Taking log of a feature can make extreme values more manageable for the model (turning multiplicative differences into additive ones). For example, if we had a feature where most values are 1-10 but a few are 10000, log1p would compress that 10000 down to ~9.21, making it less dramatically different. |\n",
    "\n",
    "Now, let's actually create these new features in our DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370e9541",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Make a copy to add features (to keep original clean data if needed)\n",
    "df_fe = df_clean.copy()\n",
    "\n",
    "# Total counts\n",
    "df_fe['pkts_tot'] = df_fe['fwd_pkts_tot'] + df_fe['bwd_pkts_tot']\n",
    "df_fe['data_pkts_tot'] = df_fe['fwd_data_pkts_tot'] + df_fe['bwd_data_pkts_tot']\n",
    "df_fe['hdr_size_tot_sum'] = df_fe['fwd_header_size_tot'] + df_fe['bwd_header_size_tot']\n",
    "\n",
    "# Ratios and symmetry between forward/backward\n",
    "df_fe['pkts_ratio_fb'] = (df_fe['fwd_pkts_tot'] + 1) / (df_fe['bwd_pkts_tot'] + 1)\n",
    "df_fe['data_pkts_ratio_fb'] = (df_fe['fwd_data_pkts_tot'] + 1) / (df_fe['bwd_data_pkts_tot'] + 1)\n",
    "df_fe['hdr_size_ratio_fb'] = (df_fe['fwd_header_size_tot'] + 1) / (df_fe['bwd_header_size_tot'] + 1)\n",
    "df_fe['pkts_symmetry'] = (df_fe['fwd_pkts_tot'] - df_fe['bwd_pkts_tot']) / (df_fe['fwd_pkts_tot'] + df_fe['bwd_pkts_tot'] + 1e-9)\n",
    "df_fe['hdr_size_symmetry'] = (df_fe['fwd_header_size_tot'] - df_fe['bwd_header_size_tot']) / (df_fe['fwd_header_size_tot'] + df_fe['bwd_header_size_tot'] + 1e-9)\n",
    "\n",
    "# Burstiness: coefficient of variation for inter-arrival times\n",
    "df_fe['fwd_iat_cv'] = df_fe['fwd_iat_std'] / (df_fe['fwd_iat_avg'] + 1e-9)\n",
    "df_fe['bwd_iat_cv'] = df_fe['bwd_iat_std'] / (df_fe['bwd_iat_avg'] + 1e-9)\n",
    "df_fe['flow_iat_cv'] = df_fe['flow_iat_std'] / (df_fe['flow_iat_avg'] + 1e-9)\n",
    "\n",
    "# Active vs idle time share\n",
    "df_fe['active_share'] = df_fe['active_tot'] / (df_fe['active_tot'] + df_fe['idle_tot'] + 1e-9)\n",
    "df_fe['idle_share'] = df_fe['idle_tot'] / (df_fe['active_tot'] + df_fe['idle_tot'] + 1e-9)\n",
    "\n",
    "# Flag densities (as an example, we'll do SYN and ACK flags; similar can be done for others)\n",
    "df_fe['flow_syn_flag_count_density'] = df_fe['flow_syn_flag_count'] / (df_fe['pkts_tot'] + 1e-9)\n",
    "df_fe['flow_ack_flag_count_density'] = df_fe['flow_ack_flag_count'] / (df_fe['pkts_tot'] + 1e-9)\n",
    "\n",
    "# TCP window size change\n",
    "df_fe['fwd_window_delta'] = df_fe['fwd_last_window_size'] - df_fe['fwd_init_window_size']\n",
    "\n",
    "print(\"Added new features. New shape:\", df_fe.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67077ee",
   "metadata": {},
   "source": [
    "*We added a bunch of new columns to `df_fe`. The printed new shape will show an increase in the number of columns. For example, if we had 20 columns before, we might now have around 30+ columns.*\n",
    "\n",
    "*(Note: We still have the original columns like `fwd_pkts_tot`, `idle_tot`, etc. We could drop some original ones that have been replaced by new features – for instance, after creating `active_share` and `idle_share`, the raw `active_tot` and `idle_tot` might not be needed. However, we can keep them for now or drop them later if we want to avoid feeding redundant information to the model.)*\n",
    "\n",
    "## 10. Save the Cleaned Dataset\n",
    "\n",
    "1. Finally, we save our cleaned and feature-enhanced dataset so we can use it in the modeling stage (next notebook) without repeating all these steps. We’ll save it into the `data/processed/` folder. By saving, we ensure reproducibility (we can always load this processed data) and keep our raw data untouched.\n",
    "\n",
    "We’ll use a SQLite database to save the DataFrame as a table (alternatively, we could use a CSV or pickle file, but a database table is handy for structured data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6accf26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned & feature-engineered data to a processed SQLite database\n",
    "with sqlite3.connect(Path(\"data/processed/rt_iot2022_processed.db\")) as conn:\n",
    "    df_fe.to_sql(\"flows_clean\", conn, if_exists=\"replace\", index=False)\n",
    "\n",
    "print(\"Saved cleaned data to data/processed/rt_iot2022_processed.db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9d23d9",
   "metadata": {},
   "source": [
    "*This writes `df_fe` to a SQLite database file named `rt_iot2022_processed.db` under the table name `\"flows_clean\"`. Later, our modeling notebook will load this table.*\n",
    "\n",
    "*(By saving now, we **freeze** our preprocessing results. The raw data remains unchanged in `data/raw/`, and any future analysis or modeling will start from this cleaned dataset we just saved.)*\n",
    "\n",
    "---\n",
    "\n",
    "**Conclusion:** After all these steps, our data is much **cleaner, smaller, and richer in information** than what we started with. We removed unhelpful or misleading features, dealt with anomalies like duplicates and constant columns, and even created new features that capture important patterns (like ratios and shares). Now our dataset is ready for the next stage: Machine learning.\n",
    "\n",
    "Next file: 02. IC2_regression_IoT_Preprocessing.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
