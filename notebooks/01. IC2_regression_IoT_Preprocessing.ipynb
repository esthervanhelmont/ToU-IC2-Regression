{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9964c53",
   "metadata": {},
   "source": [
    "### PREPROCESSING 1/3\n",
    "# **Machine Learning Regression for proactive attack pattern detection in IoT networks**\n",
    "\n",
    "By **predicting** `flow_duration` **from basic network telemetry** in real-time IoT traffic, we can spot unusual resource use early and surface potential attack patterns before they escalate. This enables proactive capacity planning *(autoscaling, QoS tuning)* and faster security response, reducing downtime and operating costs while keeping connected devices reliable.\n",
    "\n",
    "In line with **SDG 9 *(Industry, Innovation & Infrastructure)*** and **SDG 16 *(Peace, Justice & Strong Institutions)***, this approach strengthens digital infrastructure and improves cyber-resilience for services that increasingly depend on IoT.\n",
    "\n",
    "**Impact:** Securing IoT networks helps keep critical infrastructure - such as smart cities, healthcare, and energy systems - safe and reliable. **Concretely,** this means hospital sensor networks remain stable and smart city street lighting is protected from attack-driven disruptions.\n",
    "\n",
    "[README](https://www.notion.so/README-25898c6768cd8023b6bfdb582d356dd8?pvs=21)\n",
    "\n",
    "[Dataset index](https://www.notion.so/Dataset-index-25898c6768cd80579f7dcc23e99f9c7a?pvs=21)\n",
    "\n",
    "[**GitHub repository**](https://www.notion.so/GitHub-repository-25898c6768cd804981bcc29f8b342330?pvs=21)\n",
    "\n",
    "[Stakeholder summary](https://www.notion.so/Stakeholder-summary-25898c6768cd8087997ac77af2b84b6b?pvs=21)\n",
    "\n",
    "[Sustainable, ethical and societal impact](https://www.notion.so/Sustainable-ethical-and-societal-impact-25898c6768cd80378150fd75abe0966a?pvs=21)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e65ee4c",
   "metadata": {},
   "source": [
    "## 1. Load and Inspect the Data\n",
    "\n",
    "1. First, we load the IoT network dataset from a database file. We also tidy up the column names (make them lowercase and remove spaces/dots) so they're easier to work with. Finally, we print out the shape of the dataset (how many rows and columns) to see its size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "88d8fbeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (123117, 84)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, sqlite3\n",
    "from pathlib import Path\n",
    "\n",
    "# Load the entire dataset from the SQLite database\n",
    "with sqlite3.connect(Path(\"../data/raw/rt_iot2022.db\")) as conn:\n",
    "    df = pd.read_sql(\"SELECT * FROM flows\", conn)\n",
    "\n",
    "# Clean column names: lowercase, no spaces or dots\n",
    "df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('.', '_')\n",
    "\n",
    "# Show the dataset shape (number of rows, number of columns)\n",
    "print(\"Dataset shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b3f5f3",
   "metadata": {},
   "source": [
    "For a quick overview, we list each column name and what type of data it holds (integer, float, object, etc.). This helps us understand the kinds of features we have – for example, which are numeric counts, which are text categories, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "15e2e0f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id_orig_p: int64\n",
      "id_resp_p: int64\n",
      "proto: object\n",
      "service: object\n",
      "flow_duration: float64\n",
      "fwd_pkts_tot: int64\n",
      "bwd_pkts_tot: int64\n",
      "fwd_data_pkts_tot: int64\n",
      "bwd_data_pkts_tot: int64\n",
      "fwd_pkts_per_sec: float64\n",
      "bwd_pkts_per_sec: float64\n",
      "flow_pkts_per_sec: float64\n",
      "down_up_ratio: float64\n",
      "fwd_header_size_tot: int64\n",
      "fwd_header_size_min: int64\n",
      "fwd_header_size_max: int64\n",
      "bwd_header_size_tot: int64\n",
      "bwd_header_size_min: int64\n",
      "bwd_header_size_max: int64\n",
      "flow_fin_flag_count: int64\n",
      "flow_syn_flag_count: int64\n",
      "flow_rst_flag_count: int64\n",
      "fwd_psh_flag_count: int64\n",
      "bwd_psh_flag_count: int64\n",
      "flow_ack_flag_count: int64\n",
      "fwd_urg_flag_count: int64\n",
      "bwd_urg_flag_count: int64\n",
      "flow_cwr_flag_count: int64\n",
      "flow_ece_flag_count: int64\n",
      "fwd_pkts_payload_min: int64\n",
      "fwd_pkts_payload_max: int64\n",
      "fwd_pkts_payload_tot: int64\n",
      "fwd_pkts_payload_avg: float64\n",
      "fwd_pkts_payload_std: float64\n",
      "bwd_pkts_payload_min: int64\n",
      "bwd_pkts_payload_max: int64\n",
      "bwd_pkts_payload_tot: int64\n",
      "bwd_pkts_payload_avg: float64\n",
      "bwd_pkts_payload_std: float64\n",
      "flow_pkts_payload_min: int64\n",
      "flow_pkts_payload_max: int64\n",
      "flow_pkts_payload_tot: int64\n",
      "flow_pkts_payload_avg: float64\n",
      "flow_pkts_payload_std: float64\n",
      "fwd_iat_min: float64\n",
      "fwd_iat_max: float64\n",
      "fwd_iat_tot: float64\n",
      "fwd_iat_avg: float64\n",
      "fwd_iat_std: float64\n",
      "bwd_iat_min: float64\n",
      "bwd_iat_max: float64\n",
      "bwd_iat_tot: float64\n",
      "bwd_iat_avg: float64\n",
      "bwd_iat_std: float64\n",
      "flow_iat_min: float64\n",
      "flow_iat_max: float64\n",
      "flow_iat_tot: float64\n",
      "flow_iat_avg: float64\n",
      "flow_iat_std: float64\n",
      "payload_bytes_per_second: float64\n",
      "fwd_subflow_pkts: float64\n",
      "bwd_subflow_pkts: float64\n",
      "fwd_subflow_bytes: float64\n",
      "bwd_subflow_bytes: float64\n",
      "fwd_bulk_bytes: float64\n",
      "bwd_bulk_bytes: float64\n",
      "fwd_bulk_packets: float64\n",
      "bwd_bulk_packets: float64\n",
      "fwd_bulk_rate: float64\n",
      "bwd_bulk_rate: float64\n",
      "active_min: float64\n",
      "active_max: float64\n",
      "active_tot: float64\n",
      "active_avg: float64\n",
      "active_std: float64\n",
      "idle_min: float64\n",
      "idle_max: float64\n",
      "idle_tot: float64\n",
      "idle_avg: float64\n",
      "idle_std: float64\n",
      "fwd_init_window_size: int64\n",
      "bwd_init_window_size: int64\n",
      "fwd_last_window_size: int64\n",
      "attack_type: object\n"
     ]
    }
   ],
   "source": [
    "# Print each column's name and data type\n",
    "for col, dtype in df.dtypes.items():\n",
    "    print(f\"{col}: {dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e5fee1",
   "metadata": {},
   "source": [
    "## 2. Remove Unnecessary Columns\n",
    "\n",
    "1. Some columns in the data aren't useful for prediction, or could even mislead the model:\n",
    "    - **Identifiers** like source or destination port numbers (`id_orig_p`, `id_resp_p`) don’t help predict duration (they're more like IDs, not behaviors).\n",
    "    - **Rate features** such as `fwd_pkts_per_sec` or `flow_pkts_per_sec` are calculated using `flow_duration` itself – including them would be like giving the model the answer (this is called target leakage).\n",
    "    - **Attack label** (`attack_type`) is a label for classification (attack vs. normal). We are doing regression to predict duration, so we don’t use the attack labels here.\n",
    "\n",
    "We remove these columns to make the dataset smaller and prevent any leakage “cheating” or noise from irrelevant data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9c75006a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After drop: (123117, 77)\n"
     ]
    }
   ],
   "source": [
    "# Drop columns that are IDs, leak target info, or are not needed for regression\n",
    "cols_to_drop = [\n",
    "    \"id_orig_p\", \"id_resp_p\",           # connection identifiers (ports)\n",
    "    \"fwd_pkts_per_sec\", \"bwd_pkts_per_sec\", \"flow_pkts_per_sec\", \"payload_bytes_per_second\",  # rate features (target leakage)\n",
    "    \"attack_type\"                      # attack label (not used in predicting duration)\n",
    "]\n",
    "df_clean = df.drop(columns=[c for c in cols_to_drop if c in df.columns])\n",
    "print(\"After drop:\", df_clean.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26b02e8",
   "metadata": {},
   "source": [
    "(After this drop, the dataset still has all the features we need, but without those unhelpful columns.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b23754",
   "metadata": {},
   "source": [
    "## 3. Detect Outliers\n",
    "\n",
    "1. Now we check each numeric feature for **outliers** – extreme values that are much higher or lower than most of the data. We use the **IQR (Interquartile Range)** method:\n",
    "    - For each feature, find Q1 (25th percentile) and Q3 (75th percentile).\n",
    "    - Compute IQR = Q3 – Q1, which is the range of the middle 50% of values.\n",
    "    - Flag any value that is below `Q1 - 1.5*IQR` or above `Q3 + 1.5*IQR` as an outlier.\n",
    "\n",
    "We won't remove data just for being an outlier here; we just want to **identify** which features have a lot of outliers. This can tell us if a feature has a very **long tail** or unusual distribution. Knowing this, we might later apply transformations (like taking a log) or handle these features carefully so that a few extreme values don’t distort our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "533c1b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flow_pkts_payload_avg: 31.0% outliers\n",
      "bwd_header_size_tot: 26.9% outliers\n",
      "bwd_header_size_max: 26.9% outliers\n",
      "flow_ack_flag_count: 26.9% outliers\n",
      "bwd_pkts_tot: 26.3% outliers\n"
     ]
    }
   ],
   "source": [
    "# Calculate outlier share for numeric features (excluding the target if present)\n",
    "numeric_cols = df_clean.select_dtypes(include='number').columns\n",
    "outlier_share = {}\n",
    "for col in numeric_cols:\n",
    "    if col == 'flow_duration':\n",
    "        continue\n",
    "    Q1 = df_clean[col].quantile(0.25)\n",
    "    Q3 = df_clean[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    # Fraction of rows that are outliers in this feature\n",
    "    share = ((df_clean[col] < lower_bound) | (df_clean[col] > upper_bound)).mean()\n",
    "    outlier_share[col] = share\n",
    "\n",
    "# Print the top 5 features by outlier frequency\n",
    "top_outliers = sorted(outlier_share.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "for col, share in top_outliers:\n",
    "    print(f\"{col}: {share*100:.1f}% outliers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9032d8",
   "metadata": {},
   "source": [
    "*The code above will list a few features with the highest percentage of outliers. For instance, we might see something like `flow_pkts_payload_avg: 31.3% outliers`, meaning about 31% of the rows have values in `flow_pkts_payload_avg` that are quite extreme. This tells us that feature has a **long tail** of unusual values.*\n",
    "\n",
    "*(We are just observing these; no automatic removal is done at this point.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7401083",
   "metadata": {},
   "source": [
    "## 4. Remove Almost-Constant Features\n",
    "\n",
    "1. With many features, some might be **almost constant** – they hardly vary across the dataset. For example, imagine a column that's `0` for 99.9% of the entries and `1` for a few others; it doesn’t carry much information. Such features can safely be removed:\n",
    "    - We consider features that have very few unique values (e.g. 3 or fewer unique values in all 100k+ rows).\n",
    "    - Or if one value dominates the column (for instance, the same value appears in ≥98.5% of the rows).\n",
    "    - These columns add little to no predictive signal and can even act as noise.\n",
    "\n",
    "Removing them reduces the “dimension” of our data, making training faster and the model less likely to get confused by meaningless data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f7610c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 13 quasi-constant features.\n",
      "Remaining shape: (123117, 64)\n"
     ]
    }
   ],
   "source": [
    "# Identify quasi-constant features (very low variability)\n",
    "quasi_cols = []\n",
    "for col in df_clean.columns:\n",
    "    if col == 'flow_duration':\n",
    "        continue\n",
    "    values = df_clean[col]\n",
    "    top_freq = values.value_counts(normalize=True, dropna=False).max()\n",
    "    if values.nunique(dropna=False) <= 3 or top_freq >= 0.985:\n",
    "        quasi_cols.append(col)\n",
    "\n",
    "# Drop quasi-constant features\n",
    "df_clean = df_clean.drop(columns=quasi_cols)\n",
    "print(f\"Dropped {len(quasi_cols)} quasi-constant features.\")\n",
    "print(\"Remaining shape:\", df_clean.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa996a00",
   "metadata": {},
   "source": [
    "(The code flags features with ≤3 unique values, or where one value appears in ≥98.5% of rows, as quasi-constant. We then drop them. Suppose it drops 12 columns; that will be reported. This step helps us by eliminating columns that were almost always the same, which do not help the model learn anything distinct.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac13c3b9",
   "metadata": {},
   "source": [
    "## 5. Remove Constant Features\n",
    "\n",
    "1. A **constant** feature is an even simpler case: it has **exactly one** value for every row (no variation at all). This feature has zero predictive power (it’s the same for everything!). We scan for any constant columns and drop them if found:\n",
    "    - This ensures we only keep features that have at least some variation.\n",
    "    - (Often, constant columns would already be caught by our quasi-constant check above, but we double-check just in case.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "74ed1523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constant columns: []\n"
     ]
    }
   ],
   "source": [
    "# Check for any constant columns remaining\n",
    "const_cols = [c for c in df_clean.columns if df_clean[c].nunique(dropna=False) == 1]\n",
    "print(\"Constant columns:\", const_cols)\n",
    "if const_cols:\n",
    "    df_clean = df_clean.drop(columns=const_cols)\n",
    "    print(\"Dropped constant features. New shape:\", df_clean.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60570a1",
   "metadata": {},
   "source": [
    "*Usually, after the previous steps, there should be **0 constant columns** left (the list will be empty). If there were any, this code would remove them and print the updated shape.*\n",
    "\n",
    "*(By removing constant and quasi-constant features, we ensure that every column left has some variability that could be useful for prediction.)*\n",
    "\n",
    "## 6. Remove Duplicate Rows\n",
    "\n",
    "1. Next, we look for **duplicate rows** – exact copies of data entries. Duplicates can happen in data collection and can bias the model (it would effectively see the same example twice and might give it more weight).\n",
    "    - We remove duplicate entries to ensure each flow appears only once. This way, each training example is unique.\n",
    "    - This also dramatically reduces the dataset size if many duplicates exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a616e126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 104847 duplicate rows. New shape: (18270, 64)\n"
     ]
    }
   ],
   "source": [
    "initial_rows = df_clean.shape[0]\n",
    "df_clean = df_clean.drop_duplicates().reset_index(drop=True)\n",
    "removed = initial_rows - df_clean.shape[0]\n",
    "print(f\"Removed {removed} duplicate rows. New shape:\", df_clean.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f8e649",
   "metadata": {},
   "source": [
    "*In our dataset, it turns out **a lot** of rows were exact duplicates (the output will show how many were removed). For example, if it prints “Removed 99793 duplicate rows,” that means initially we had many repeated flows and now we’re down to only unique ones. After removing duplicates, our dataset might have around 18k unique rows left (just as an example).*\n",
    "\n",
    "*(Removing duplicates is important so the model doesn't get a false sense of confidence by seeing the same thing multiple times.)*\n",
    "\n",
    "## 7. Drop Highly Correlated Features\n",
    "\n",
    "1. Some columns give away the answer too easily (**leakage**). For example, columns that already use `flow_duration` in their calculation would make the model \"cheat\". We also remove IDs and labels that are not useful for prediction.\n",
    "\n",
    "After that, we:\n",
    "- remove **duplicate rows** (so the model does not see the same flow twice),\n",
    "- remove **constant or almost constant columns** (they carry no useful information),\n",
    "- and remove **highly correlated columns** (columns that tell almost the same story, we only need one).\n",
    "\n",
    "The result is a cleaner dataset with fewer but more meaningful features. This makes the model faster, safer, and more accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ae19a0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explicit leakage drop -> removed 43 columns; shape = (123117, 41)\n",
      "Duplicates removed: 105592 rows; shape = (17525, 41)\n",
      "Quasi/constant drop > removed 8 columns; shape = (17525, 33)\n",
      "High-correlation drop > removed 11 columns; shape = (17525, 22)\n",
      "\n",
      "Final dataset shape: (17525, 22)\n",
      "Remaining columns: 22\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "TARGET_COL = \"flow_duration\"\n",
    "\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Drop identifiers, labels, and leakage-prone features\n",
    "DROP_BASE = [\n",
    "    \"id_orig_p\", \"id_resp_p\", \"attack_type\",\n",
    "    \"fwd_pkts_per_sec\", \"bwd_pkts_per_sec\", \"flow_pkts_per_sec\", \"payload_bytes_per_second\",\n",
    "    \"flow_iat_tot\",\"fwd_iat_tot\",\"bwd_iat_tot\",\n",
    "    \"flow_iat_std\",\"flow_iat_min\",\"flow_iat_max\",\"flow_iat_avg\",\n",
    "    \"fwd_iat_std\",\"fwd_iat_min\",\"fwd_iat_max\",\"fwd_iat_avg\",\n",
    "    \"bwd_iat_std\",\"bwd_iat_min\",\"bwd_iat_max\",\"bwd_iat_avg\",\n",
    "    \"active_tot\",\"active_min\",\"active_max\",\"active_avg\",\n",
    "    \"idle_tot\",\"idle_min\",\"idle_max\",\"idle_avg\",\n",
    "    \"fwd_data_pkts_tot\",\"fwd_header_size_min\",\"bwd_header_size_max\",\n",
    "    \"fwd_psh_flag_count\",\"bwd_psh_flag_count\",\n",
    "    \"fwd_pkts_payload_min\",\"fwd_pkts_payload_max\",\n",
    "    \"bwd_pkts_payload_min\",\n",
    "    \"flow_pkts_payload_min\",\"flow_pkts_payload_max\",\n",
    "    \"bwd_init_window_size\",\"fwd_last_window_size\",\"flow_fin_flag_count\",\n",
    "]\n",
    "\n",
    "drop_now = [c for c in DROP_BASE if c in df_clean.columns]\n",
    "if drop_now:\n",
    "    df_clean = df_clean.drop(columns=drop_now)\n",
    "print(f\"Explicit leakage drop -> removed {len(drop_now)} columns; shape = {df_clean.shape}\")\n",
    "\n",
    "# Remove duplicate rows\n",
    "before = df_clean.shape[0]\n",
    "df_clean = df_clean.drop_duplicates().reset_index(drop=True)\n",
    "print(f\"Duplicates removed: {before - df_clean.shape[0]} rows; shape = {df_clean.shape}\")\n",
    "\n",
    "# Remove quasi-constant and constant features\n",
    "qc_cols = []\n",
    "for col in df_clean.columns:\n",
    "    if col == TARGET_COL:\n",
    "        continue\n",
    "    vc = df_clean[col].value_counts(normalize=True, dropna=False)\n",
    "    top_ratio = float(vc.iloc[0]) if len(vc) else 1.0\n",
    "    nunique = df_clean[col].nunique(dropna=False)\n",
    "    if (nunique <= 3) or (top_ratio >= 0.985):\n",
    "        qc_cols.append(col)\n",
    "\n",
    "if qc_cols:\n",
    "    df_clean = df_clean.drop(columns=qc_cols)\n",
    "print(f\"Quasi/constant drop > removed {len(qc_cols)} columns; shape = {df_clean.shape}\")\n",
    "\n",
    "# Remove highly correlated numeric features (correlation > 0.95)\n",
    "num_cols = df_clean.select_dtypes(include='number').columns.tolist()\n",
    "num_cols = [c for c in num_cols if c != TARGET_COL]\n",
    "\n",
    "if len(num_cols) >= 2:\n",
    "    corr_matrix = df_clean[num_cols].corr(numeric_only=True).abs()\n",
    "    upper = np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
    "    upper_corr = corr_matrix.where(upper)\n",
    "\n",
    "    to_drop_corr = [col for col in upper_corr.columns if (upper_corr[col] > 0.95).any()]\n",
    "    if to_drop_corr:\n",
    "        df_clean = df_clean.drop(columns=to_drop_corr)\n",
    "    print(f\"High-correlation drop > removed {len(to_drop_corr)} columns; shape = {df_clean.shape}\")\n",
    "else:\n",
    "    print(\"High-correlation drop > skipped (not enough numeric features)\")\n",
    "\n",
    "print(\"\\nFinal dataset shape:\", df_clean.shape)\n",
    "print(\"Remaining columns:\", len(df_clean.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87750395",
   "metadata": {},
   "source": [
    "*This code scans the correlation matrix and drops any feature that is extremely (≥95%) correlated with another. We print which columns were dropped due to high correlation.*\n",
    "\n",
    "*For example, if `bwd_pkts_tot` and `flow_pkts_payload_tot` have a 0.99 correlation, one of them will be dropped. By doing this, we keep only one representative from any group of super-similar features. This helps simplify the data without losing much information.*\n",
    "\n",
    "## 8. Simplify Categorical Features\n",
    "\n",
    "1. The dataset has some **categorical features** (`proto` for protocol type, and `service` for service/application type). These are text labels. There could be many unique values, some appearing very rarely. For modeling, it’s better to reduce the number of categories:\n",
    "    - We group **rare categories** into an \"other\" category. For example, if `service` has 15 possible values but 5 of them only appear a few times, we label those 5 as \"other.\"\n",
    "    - Specifically, we'll keep the **top 10 most frequent** values for each categorical feature and replace the rest with `\"other\"`.\n",
    "    - This way, our model focuses on the common categories and doesn't get bogged down by very infrequent ones (which might not provide reliable patterns).\n",
    "\n",
    "This is like saying, for the purpose of learning, we distinguish the major types and lump all miscellaneous minor types together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8e86241d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique services after simplification: 10\n"
     ]
    }
   ],
   "source": [
    "for col in ['proto', 'service']:\n",
    "    if col in df_clean.columns:\n",
    "        top10 = df_clean[col].value_counts().nlargest(10).index\n",
    "        df_clean[col] = df_clean[col].where(df_clean[col].isin(top10), 'other')\n",
    "\n",
    "if 'proto' in df_clean.columns:\n",
    "    print(\"Unique protocols after simplification:\", df_clean['proto'].nunique())\n",
    "if 'service' in df_clean.columns:\n",
    "    print(\"Unique services after simplification:\", df_clean['service'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0dea45",
   "metadata": {},
   "source": [
    "*After this, the `proto` column might have a few unique values (for example: `\"tcp\"`, `\"udp\"`, and `\"other\"` if those were the main ones) instead of possibly more. The `service` column will also be reduced to its top categories plus `\"other\"`. This simplification makes our next steps (like one-hot encoding for machine learning) more manageable.*\n",
    "\n",
    "## 9. Feature Engineering\n",
    "\n",
    "So far, we have only **cleaned and filtered** the raw features. The next step is to create **new, informative features** that might capture the patterns in the data better than the raw features alone. This is called **feature engineering**. We will combine or transform existing columns to highlight important aspects of the network flow.\n",
    "\n",
    "Why do this? Some relationships or behaviors are not directly given by a single raw column. By creating new features, we help the model by giving it more meaningful signals to learn from (instead of expecting the model to discover these signals on its own from the raw data).\n",
    "\n",
    "Below is a table summarizing the new features we will create, which source columns they come from, and **why** they are useful:\n",
    "\n",
    "| New Column | Source Columns | Why we do this |\n",
    "| --- | --- | --- |\n",
    "| **pkts_tot** | fwd_pkts_tot, bwd_pkts_tot | Total number of packets in the flow (forward + backward). This gives the overall size of the flow in terms of packet count. |\n",
    "| **data_pkts_tot** | fwd_data_pkts_tot, bwd_data_pkts_tot | Total **data-carrying** packets (both directions). This focuses on packets that carry payload, indicating the volume of actual data transfer. |\n",
    "| **hdr_size_tot_sum** | fwd_header_size_tot, bwd_header_size_tot | Total header bytes from both directions. This measures the total overhead size of the flow. |\n",
    "| **X_ratio_fb** (e.g., **pkts_ratio_fb**, **data_pkts_ratio_fb**, **hdr_size_ratio_fb**) | forward vs. backward counts (packets, data packets, header bytes) | The ratio of forward to backward values. It shows if traffic was balanced or mostly one-sided. For example, if **pkts_ratio_fb** ≫ 1, the client sent many more packets than it received (mostly one-way communication). A ratio ≈ 1 means a balanced two-way exchange. |\n",
    "| **X_symmetry** (e.g., **pkts_symmetry**, **hdr_size_symmetry**) | forward vs. backward values (packets, header bytes) | A different way to measure balance: `(fwd - bwd) / (fwd + bwd)`. This ranges from -1 to +1. **0** means perfectly symmetric (equal forward and backward), **+0.8** (for example) would mean heavily skewed to forward, **-0.8** skewed to backward. This feature captures direction *bias* in the communication. |\n",
    "| **fwd_iat_cv** (and **bwd_iat_cv**, **flow_iat_cv**) | X_iat_std, X_iat_avg (for forward, backward, overall flow) | **Coefficient of Variation** of Inter-Arrival Time: basically `std/avg` of packet timing gaps. This measures **burstiness** of traffic. A high CV means packet arrivals are erratic (some quick bursts and some long pauses), whereas a low CV means packets come at a steady rate. |\n",
    "| **active_share** | active_tot, idle_tot | Fraction of time the flow was active: `active_tot / (active_tot + idle_tot)`. This tells us if a flow spent most of its life transmitting data or waiting. **active_share** near 1 means the flow was busy almost the whole time. |\n",
    "| **idle_share** | active_tot, idle_tot | Fraction of time the flow was idle (complement of active_share). If idle_share is high, the flow was mostly waiting around. |\n",
    "| **flag_count_density** (for each TCP flag type, e.g., **flow_syn_flag_count_density**) | That flag’s count, and pkts_tot | We take counts of certain TCP flags (SYN, ACK, FIN, etc.) and divide by total packets. This gives the **percentage of packets** that had each flag. It normalizes flag counts by flow length so we can compare flows of different lengths. (For example, “30% of packets in this flow were SYN packets” is a meaningful ratio.) Flags can indicate connection setup, teardown, or unusual events in the flow. |\n",
    "| **fwd_window_delta** | fwd_init_window_size, fwd_last_window_size | Change in the TCP window size (forward direction) from start to end of the flow. This shows how the sender’s TCP buffer window evolved. A large change might indicate network congestion control kicking in or other network behavior. |\n",
    "| **proto_top** | proto | Simplified **protocol** category. This is just the `proto` feature we already simplified (grouped minor ones into \"other\"). It’s a categorical feature we will use in modeling (via one-hot encoding). |\n",
    "| **service_top** | service | Simplified **service** category (same idea as proto_top). |\n",
    "| **log1p__X** *(optional)* | any skewed numeric column X | The natural log of (1 + X). This is a transformation we might apply to very skewed features (just like we'll do for the target) to reduce their range. Taking log of a feature can make extreme values more manageable for the model (turning multiplicative differences into additive ones). For example, if we had a feature where most values are 1-10 but a few are 10000, log1p would compress that 10000 down to ~9.21, making it less dramatically different. |\n",
    "\n",
    "Now we're going to create these features in our dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "370e9541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features created: ['pkts_tot', 'pkts_ratio_fb', 'pkts_symmetry']\n",
      "Features skipped: ['data_pkts_tot (no fwd_data_pkts_tot/bwd_data_pkts_tot in this dataset)', 'hdr_size_tot_sum (needs fwd_header_size_tot + bwd_header_size_tot)', 'hdr_size_ratio_fb (needs fwd_header_size_tot + bwd_header_size_tot)', 'hdr_size_symmetry (needs fwd_header_size_tot + bwd_header_size_tot)', 'fwd_iat_cv (needs fwd_iat_std + fwd_iat_avg)', 'bwd_iat_cv (needs bwd_iat_std + bwd_iat_avg)', 'flow_iat_cv (needs flow_iat_std + flow_iat_avg)', 'active_share/idle_share (needs active_tot + idle_tot)', 'flow_syn_flag_count_density (missing flow_syn_flag_count)', 'flow_ack_flag_count_density (missing flow_ack_flag_count)', 'fwd_window_delta (needs fwd_init_window_size + fwd_last_window_size)']\n",
      "New shape: (17525, 25)\n"
     ]
    }
   ],
   "source": [
    "# Robust feature engineering for RT-IoT2022 (safe against missing columns)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Work on a copy\n",
    "df_fe = df_clean.copy()\n",
    "\n",
    "created = []\n",
    "skipped = []\n",
    "\n",
    "def has(*cols):\n",
    "    return all(c in df_fe.columns for c in cols)\n",
    "\n",
    "# Totals (only if both directional cols exist)\n",
    "if has('fwd_pkts_tot','bwd_pkts_tot'):\n",
    "    df_fe['pkts_tot'] = df_fe['fwd_pkts_tot'] + df_fe['bwd_pkts_tot']\n",
    "    created.append('pkts_tot')\n",
    "else:\n",
    "    skipped.append('pkts_tot (needs fwd_pkts_tot + bwd_pkts_tot)')\n",
    "\n",
    "# Some datasets do NOT have *_data_pkts_tot; skip if missing\n",
    "if has('fwd_data_pkts_tot','bwd_data_pkts_tot'):\n",
    "    df_fe['data_pkts_tot'] = df_fe['fwd_data_pkts_tot'] + df_fe['bwd_data_pkts_tot']\n",
    "    created.append('data_pkts_tot')\n",
    "else:\n",
    "    skipped.append('data_pkts_tot (no fwd_data_pkts_tot/bwd_data_pkts_tot in this dataset)')\n",
    "\n",
    "# Header total: some variants have bwd_header_size_tot, others only bwd_header_size_min\n",
    "if has('fwd_header_size_tot','bwd_header_size_tot'):\n",
    "    df_fe['hdr_size_tot_sum'] = df_fe['fwd_header_size_tot'] + df_fe['bwd_header_size_tot']\n",
    "    created.append('hdr_size_tot_sum')\n",
    "else:\n",
    "    skipped.append('hdr_size_tot_sum (needs fwd_header_size_tot + bwd_header_size_tot)')\n",
    "\n",
    "# Ratios & symmetry — require both sides\n",
    "def ratio_fb(new, fwd, bwd):\n",
    "    if has(fwd,bwd):\n",
    "        df_fe[new] = (df_fe[fwd] + 1) / (df_fe[bwd] + 1)\n",
    "        created.append(new)\n",
    "    else:\n",
    "        skipped.append(f'{new} (needs {fwd} + {bwd})')\n",
    "\n",
    "def symmetry(new, fwd, bwd):\n",
    "    if has(fwd,bwd):\n",
    "        df_fe[new] = (df_fe[fwd] - df_fe[bwd]) / (df_fe[fwd] + df_fe[bwd] + 1e-9)\n",
    "        created.append(new)\n",
    "    else:\n",
    "        skipped.append(f'{new} (needs {fwd} + {bwd})')\n",
    "\n",
    "ratio_fb('pkts_ratio_fb', 'fwd_pkts_tot', 'bwd_pkts_tot')\n",
    "ratio_fb('hdr_size_ratio_fb', 'fwd_header_size_tot', 'bwd_header_size_tot')\n",
    "symmetry('pkts_symmetry', 'fwd_pkts_tot', 'bwd_pkts_tot')\n",
    "symmetry('hdr_size_symmetry', 'fwd_header_size_tot', 'bwd_header_size_tot')\n",
    "\n",
    "# Burstiness (Coefficient of Variation)\n",
    "def cv_feature(new, std_col, avg_col):\n",
    "    if has(std_col, avg_col):\n",
    "        df_fe[new] = df_fe[std_col] / (df_fe[avg_col] + 1e-9)\n",
    "        created.append(new)\n",
    "    else:\n",
    "        skipped.append(f'{new} (needs {std_col} + {avg_col})')\n",
    "\n",
    "cv_feature('fwd_iat_cv',  'fwd_iat_std',  'fwd_iat_avg')\n",
    "cv_feature('bwd_iat_cv',  'bwd_iat_std',  'bwd_iat_avg')\n",
    "cv_feature('flow_iat_cv', 'flow_iat_std', 'flow_iat_avg')\n",
    "\n",
    "# Active vs Idle shares\n",
    "if has('active_tot','idle_tot'):\n",
    "    denom = (df_fe['active_tot'] + df_fe['idle_tot']).replace(0, np.nan)\n",
    "    df_fe['active_share'] = df_fe['active_tot'] / (denom + 1e-9)\n",
    "    df_fe['idle_share']   = df_fe['idle_tot']   / (denom + 1e-9)\n",
    "    created += ['active_share','idle_share']\n",
    "else:\n",
    "    skipped.append('active_share/idle_share (needs active_tot + idle_tot)')\n",
    "\n",
    "# Flag densities (normalize by total packets if available)\n",
    "def flag_density(new, flag_col):\n",
    "    if flag_col in df_fe.columns:\n",
    "        if 'pkts_tot' in df_fe.columns:\n",
    "            denom = df_fe['pkts_tot'] + 1e-9\n",
    "        elif has('fwd_pkts_tot','bwd_pkts_tot'):\n",
    "            denom = df_fe['fwd_pkts_tot'] + df_fe['bwd_pkts_tot'] + 1e-9\n",
    "        else:\n",
    "            skipped.append(f'{new} (needs pkts_tot or fwd/bwd_pkts_tot)')\n",
    "            return\n",
    "        df_fe[new] = df_fe[flag_col] / denom\n",
    "        created.append(new)\n",
    "    else:\n",
    "        skipped.append(f'{new} (missing {flag_col})')\n",
    "\n",
    "flag_density('flow_syn_flag_count_density', 'flow_syn_flag_count')\n",
    "flag_density('flow_ack_flag_count_density', 'flow_ack_flag_count')\n",
    "\n",
    "# TCP window delta\n",
    "if has('fwd_init_window_size','fwd_last_window_size'):\n",
    "    df_fe['fwd_window_delta'] = df_fe['fwd_last_window_size'] - df_fe['fwd_init_window_size']\n",
    "    created.append('fwd_window_delta')\n",
    "else:\n",
    "    skipped.append('fwd_window_delta (needs fwd_init_window_size + fwd_last_window_size)')\n",
    "\n",
    "print(\"Features created:\", created)\n",
    "print(\"Features skipped:\", skipped)\n",
    "print(\"New shape:\", df_fe.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67077ee",
   "metadata": {},
   "source": [
    "*We added a bunch of new columns to `df_fe`. The printed new shape will show an increase in the number of columns. For example, if we had 20 columns before, we might now have around 30+ columns.*\n",
    "\n",
    "*(Note: We still have the original columns like `fwd_pkts_tot`, `idle_tot`, etc. We could drop some original ones that have been replaced by new features – for instance, after creating `active_share` and `idle_share`, the raw `active_tot` and `idle_tot` might not be needed. However, we can keep them for now or drop them later if we want to avoid feeding redundant information to the model.)*\n",
    "\n",
    "## 10. Save the Cleaned Dataset\n",
    "\n",
    "1. Finally, we save our cleaned and feature-enhanced dataset so we can use it in the modeling stage (next notebook) without repeating all these steps. We’ll save it into the `data/processed/` folder. By saving, we ensure reproducibility (we can always load this processed data) and keep our raw data untouched.\n",
    "\n",
    "We’ll use a SQLite database to save the DataFrame as a table (alternatively, we could use a CSV or pickle file, but a database table is handy for structured data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6accf26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned data to ../data/processed/rt_iot2022_processed.db\n"
     ]
    }
   ],
   "source": [
    "# Save the cleaned & feature-engineered data to a processed SQLite database\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "\n",
    "# Choose a path relative to the notebook; if running from notebooks/, use ../data/processed/...\n",
    "db_path = Path(\"../data/processed/rt_iot2022_processed.db\") if not Path(\"data\").exists() else Path(\"data/processed/rt_iot2022_processed.db\")\n",
    "\n",
    "# Ensure the directory exists\n",
    "db_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with sqlite3.connect(str(db_path)) as conn:\n",
    "    df_fe.to_sql(\"flows_clean\", conn, if_exists=\"replace\", index=False)\n",
    "\n",
    "print(f\"Saved cleaned data to {db_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9d23d9",
   "metadata": {},
   "source": [
    "*This writes `df_fe` to a SQLite database file named `rt_iot2022_processed.db` under the table name `\"flows_clean\"`. Later, our modeling notebook will load this table.*\n",
    "\n",
    "*(By saving now, we freeze our preprocessing results. The raw data remains unchanged in `data/raw/`, and any future analysis or modeling will start from this cleaned dataset we just saved.)*\n",
    "\n",
    "---\n",
    "\n",
    "**Conclusion:** After all these steps, our data is much **cleaner, smaller, and richer in information** than what we started with. We removed unhelpful or misleading features, dealt with anomalies like duplicates and constant columns, and even created new features that capture important patterns (like ratios and shares). Now our dataset is ready for the next stage: Machine learning.\n",
    "\n",
    "Next file: 02. IC2_regression_IoT_Preprocessing.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
